{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8feaf7",
   "metadata": {},
   "source": [
    "# Multi-head Self-Attention (MHSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc647d7",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f530c8",
   "metadata": {},
   "source": [
    "**TL;DR**\n",
    "\n",
    "* **Self-Attention** serves as a single person looking at a sentence and trying to understand the relationship between words. \n",
    "* **Multihead Self-Attention** is like a committee of people looking at the same sentence, where each person is assigned to look for a specific type of relationship\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "|**Feature**|**Self-Attention**|**Multi-Head Attention**|\n",
    "|-------|----------------------------|---------------------------|\n",
    "|Perspective|View the sequence through a single \"lens.\" <br> Smooth out details.|Views the sequence through multiple \"lenses\" simultaneously. <br> Focus on different aspects|\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7232f92",
   "metadata": {},
   "source": [
    "## Multi-head Self-Attention with Causal Masking and Rope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c7aab",
   "metadata": {},
   "source": [
    "**Multihead Self-Attention or MHSA** stands as the quintessential architectural pivot within the Transformer paradigm, transcending the limitations of monolithic attention mechanisms. While self-attention facilitates the mapping of dependencies within a singular sequence, MHSA extrapolates this capability by partitioning the input into diverse \"representation subspaces,\" thereby enabling the model to zero in on disparate relational features simultaneously. The act of splitting up the attention mechanism serves as a safeguard against \"representational collapse.\" By forcing the model to attend to information from different subspaces at different positions, MHSA ensures the final output is a sophisticated amalgam of varied contextual cues, rather than a flattened, singular interpretation. Let $X \\in \\mathbb{R}^{n \\times d_{model}}$ represent your input sequence, where $n$ is the sequence length and $d_{model}$ is the embedding dimension.1. \n",
    "\n",
    "**1.Linear Projections**\n",
    "\n",
    "First, we project the input $X$ into Query ($Q$), Key ($K$), and Value ($V$) spaces using learnable weight matrices. To optimize computation, we perform one large matrix multiplication for each:\n",
    "\n",
    "$$\n",
    "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "$$\n",
    "\n",
    "Where $W_Q, W_K \\in \\mathbb{R}^{d_{model} \\times (h \\cdot d_k)}$ and $W_V \\in \\mathbb{R}^{d_{model} \\times (h \\cdot d_v)}$. Here, $h$ is the number of heads, and $d_k, d_v$ are the dimensions per head.\n",
    "\n",
    "**2.Multi-Head Splitting**\n",
    "\n",
    "We reshape $Q, K, V$ to separate the heads. This effectively treats the head dimension as a batch dimension by simply $d_{model}$ divided by $h$:\n",
    "\n",
    "$$\n",
    "Q_i, K_i, V_i \\in \\mathbb{R}^{n \\times d_k} \\quad \\text{for each head } i \\in \\{1, \\dots, h\\}\n",
    "$$\n",
    "\n",
    "**3.Applying Rotary Positional Embeddings (RoPE)**\n",
    "\n",
    "For a vector at position $m$ in the sequence, the RoPE transformation $f_{\\text{RoPE}}$ is applied:\n",
    "\n",
    "$$\n",
    "\\tilde{Q}_i^{(m)} = f_{\\text{RoPE}}(Q_i^{(m)}, m), \\quad \\tilde{K}_i^{(m)} = f_{\\text{RoPE}}(K_i^{(m)}, m)\n",
    "$$\n",
    "\n",
    "**4.Scaled Dot-Product Attention with Causal Masking**\n",
    "\n",
    "For each head, we compute the attention scores and apply a causal mask $M$. The attention score matrix for head $i$ is:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Softmax}\\left(\\frac{\\tilde{Q}_i \\tilde{K}_i^T}{\\sqrt{d_k}}\\right)V_i + M\\;\\;\\;\\;\\;\\;\\text{where}\\;\\;\\;M_{ij} = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}\n",
    "$$\n",
    "\n",
    "**5.Concatenation and Output Projection**\n",
    "\n",
    "Finally, the results from all heads are concatenated and projected back to the original model dimension using the output weight matrix $W_O \\in \\mathbb{R}^{(h \\cdot d_v) \\times d_{model}}$:\n",
    "$$\n",
    "\\text{MultiHeadSelfAttention}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W_O\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc54e85",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "import torch.nn as nn\n",
    "from cs336_basics.softmax import Softmax\n",
    "from cs336_basics.sdpa import ScaledDotProductAttention\n",
    "from cs336_basics.rope import RoPE\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttentionRoPE(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model : int,\n",
    "            num_heads : int,\n",
    "            max_seq_len : int,\n",
    "            theta : float\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        d_head = d_model // num_heads\n",
    "\n",
    "        self.rope = RoPE(\n",
    "            d_k=d_head,\n",
    "            max_seq_len=max_seq_len,\n",
    "            theta=theta,\n",
    "        )\n",
    " \n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            q_proj_weight : torch.Tensor,\n",
    "            k_proj_weight : torch.Tensor,\n",
    "            v_proj_weight : torch.Tensor,\n",
    "            o_proj_weight : torch.Tensor,\n",
    "            in_features : torch.Tensor,\n",
    "            token_positions : torch.Tensor\n",
    "        ) -> Tensor:\n",
    "\n",
    "        # Linear projections\n",
    "        q = in_features @ q_proj_weight.T\n",
    "        k = in_features @ k_proj_weight.T\n",
    "        v = in_features @ v_proj_weight.T\n",
    "\n",
    "        # Reshape and Transpose for the projections\n",
    "        def split_heads(x: Tensor):\n",
    "            *batch_dims, seq_len, d_total = x.shape\n",
    "            d_head = d_total // self.num_heads\n",
    "            return x.view(*batch_dims, seq_len, self.num_heads, d_head).transpose(-3, -2)\n",
    "\n",
    "        # Split heads\n",
    "        q = split_heads(q)\n",
    "        k = split_heads(k)\n",
    "        v = split_heads(v)\n",
    "\n",
    "        # Apply RoPE\n",
    "        *batch_dims, num_heads, seq_len, d_head = q.shape\n",
    "\n",
    "        q = q.reshape(-1, seq_len, d_head)\n",
    "        k = k.reshape(-1, seq_len, d_head)\n",
    "\n",
    "        q = self.rope(q, token_positions)\n",
    "        k = self.rope(k, token_positions)\n",
    "\n",
    "        q = q.view(*batch_dims, num_heads, seq_len, d_head)\n",
    "        k = k.view(*batch_dims, num_heads, seq_len, d_head)\n",
    "\n",
    "        # Causal mask\n",
    "        seq_len = q.shape[-2]\n",
    "        causal_mask = torch.tril(\n",
    "            torch.ones(seq_len, seq_len, device=in_features.device, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        attn_output = self.attention(q, k, v, mask=causal_mask)\n",
    "\n",
    "        # Concatenate heads\n",
    "        *batch_dims, num_heads, seq_len, d_v_head = attn_output.shape\n",
    "        combined = attn_output.transpose(-3, -2).reshape(\n",
    "            *batch_dims, seq_len, num_heads * d_v_head\n",
    "        )\n",
    "\n",
    "        return combined @ o_proj_weight.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
