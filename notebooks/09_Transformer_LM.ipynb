{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8feaf7",
   "metadata": {},
   "source": [
    "# Transformer LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f530c8",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../images/TransformerLM.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.linear import Linear\n",
    "from cs336_basics.rmsnorm import RMSNorm\n",
    "from cs336_basics.embedding import Embedding\n",
    "from cs336_basics.swinglu import SwiGLUFFN\n",
    "from cs336_basics.rope import RoPE\n",
    "from cs336_basics.softmax import Softmax\n",
    "from cs336_basics.multi_head_self_attention import MultiHeadSelfAttentionRoPE\n",
    "\n",
    "\n",
    "def run_transformer_lm(\n",
    "    vocab_size: int,\n",
    "    context_length: int,\n",
    "    d_model: int,\n",
    "    num_layers: int,\n",
    "    num_heads: int,\n",
    "    d_ff: int,\n",
    "    rope_theta: float,\n",
    "    weights: dict[str, Tensor],\n",
    "    in_indices: Int[Tensor, \" batch_size sequence_length\"],\n",
    ") -> Float[Tensor, \" batch_size sequence_length vocab_size\"]:\n",
    "    \"\"\"Given the weights of a Transformer language model and input indices,\n",
    "    return the output of running a forward pass on the input indices.\n",
    "\n",
    "    This function should use RoPE.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The number of unique items in the output vocabulary to be predicted.\n",
    "        context_length (int): The maximum number of tokens to process at once.\n",
    "        d_model (int): The dimensionality of the model embeddings and sublayer outputs.\n",
    "        num_layers (int): The number of Transformer layers to use.\n",
    "        num_heads (int): Number of heads to use in multi-headed attention. `d_model` must be\n",
    "            evenly divisible by `num_heads`.\n",
    "        d_ff (int): Dimensionality of the feed-forward inner layer (section 3.3).\n",
    "        rope_theta (float): The RoPE $Theta$ parameter.\n",
    "        weights (dict[str, Tensor]):\n",
    "            State dict of our reference implementation. {num_layers} refers to an\n",
    "            integer between `0` and `num_layers - 1` (the layer index).\n",
    "            The keys of this dictionary are:\n",
    "            - `token_embeddings.weight`\n",
    "                Token embedding matrix. Shape is (vocab_size, d_model).\n",
    "            - `layers.{num_layers}.attn.q_proj.weight`\n",
    "                The query projections for all `num_heads` attention heads.\n",
    "                Shape is (num_heads * (d_model / num_heads), d_model).\n",
    "                The rows are ordered by matrices of shape (num_heads, d_k),\n",
    "                so `attn.q_proj.weight == torch.cat([q_heads.0.weight, ..., q_heads.N.weight], dim=0)`.\n",
    "            - `layers.{num_layers}.attn.k_proj.weight`\n",
    "                The key projections for all `num_heads` attention heads.\n",
    "                Shape is (num_heads * (d_model / num_heads), d_model).\n",
    "                The rows are ordered by matrices of shape (num_heads, d_k),\n",
    "                so `attn.k_proj.weight == torch.cat([k_heads.0.weight, ..., k_heads.N.weight], dim=0)`.\n",
    "            - `layers.{num_layers}.attn.v_proj.weight`\n",
    "                The value projections for all `num_heads` attention heads.\n",
    "                Shape is (num_heads * (d_model / num_heads), d_model).\n",
    "                The rows are ordered by matrices of shape (num_heads, d_v),\n",
    "                so `attn.v_proj.weight == torch.cat([v_heads.0.weight, ..., v_heads.N.weight], dim=0)`.\n",
    "            - `layers.{num_layers}.attn.output_proj.weight`\n",
    "                Weight of the multi-head self-attention output projection\n",
    "                Shape is ((d_model / num_heads) * num_heads, d_model).\n",
    "            - `layers.{num_layers}.ln1.weight`\n",
    "                Weights of affine transform for the first RMSNorm\n",
    "                applied in the transformer block.\n",
    "                Shape is (d_model,).\n",
    "            - `layers.{num_layers}.ffn.w1.weight`\n",
    "                Weight of the first linear transformation in the FFN.\n",
    "                Shape is (d_model, d_ff).\n",
    "            - `layers.{num_layers}.ffn.w2.weight`\n",
    "                Weight of the second linear transformation in the FFN.\n",
    "                Shape is (d_ff, d_model).\n",
    "            - `layers.{num_layers}.ffn.w3.weight`\n",
    "                Weight of the third linear transformation in the FFN.\n",
    "                Shape is (d_model, d_ff).\n",
    "            - `layers.{num_layers}.ln2.weight`\n",
    "                Weights of affine transform for the second RMSNorm\n",
    "                applied in the transformer block.\n",
    "                Shape is (d_model,).\n",
    "            - `ln_final.weight`\n",
    "                Weights of affine transform for RMSNorm applied to the output of the final transformer block.\n",
    "                Shape is (d_model, ).\n",
    "            - `lm_head.weight`\n",
    "                Weights of the language model output embedding.\n",
    "                Shape is (vocab_size, d_model).\n",
    "        in_indices (Int[Tensor, \"batch_size sequence_length\"]) Tensor with input indices to run the language model on. Shape is (batch_size, sequence_length), where\n",
    "            `sequence_length` is at most `context_length`.\n",
    "\n",
    "    Returns:\n",
    "        Float[Tensor, \"batch_size sequence_length vocab_size\"]: Tensor with the predicted unnormalized\n",
    "        next-word distribution for each token.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # ── Token Embeddings ──────────────────────────────────────────────────────\n",
    "    embedding = Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "    embedding.token_ids.data = weights[\"token_embeddings.weight\"]\n",
    "\n",
    "    x = embedding(in_indices)   # (batch, seq_len, d_model)\n",
    "\n",
    "    seq_len = x.shape[1]\n",
    "    token_positions = torch.arange(seq_len, device=x.device)\n",
    "\n",
    "    # ── Transformer Layers ───────────────────────────────────────────────────\n",
    "    for layer_idx in range(num_layers):\n",
    "        pfx = f\"layers.{layer_idx}\"\n",
    "\n",
    "        # --- Sublayer 1: Pre-norm + Attention + residual ---\n",
    "        ln1 = RMSNorm(d_model=d_model)\n",
    "        ln1.weight.data = weights[f\"{pfx}.ln1.weight\"]\n",
    "\n",
    "        mhsa = MultiHeadSelfAttentionRoPE(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            max_seq_len=context_length,\n",
    "            theta=rope_theta,\n",
    "        )\n",
    "\n",
    "        q_proj_weight = weights[f\"{pfx}.attn.q_proj.weight\"]\n",
    "        k_proj_weight = weights[f\"{pfx}.attn.k_proj.weight\"]\n",
    "        v_proj_weight = weights[f\"{pfx}.attn.v_proj.weight\"]\n",
    "        o_proj_weight = weights[f\"{pfx}.attn.output_proj.weight\"]\n",
    "\n",
    "        x_norm = ln1(x)\n",
    "        attn_out = mhsa(\n",
    "            q_proj_weight,\n",
    "            k_proj_weight,\n",
    "            v_proj_weight,\n",
    "            o_proj_weight,\n",
    "            x_norm,\n",
    "            token_positions=token_positions,\n",
    "        )\n",
    "        x = x + attn_out\n",
    "\n",
    "        # --- Sublayer 2: Pre-norm + FFN + residual ---\n",
    "        ln2 = RMSNorm(d_model=d_model)\n",
    "        ln2.weight.data = weights[f\"{pfx}.ln2.weight\"]\n",
    "\n",
    "        ffn = SwiGLUFFN(d_model=d_model, d_ff=d_ff)\n",
    "        ffn.w1_weight.data = weights[f\"{pfx}.ffn.w1.weight\"]\n",
    "        ffn.w2_weight.data = weights[f\"{pfx}.ffn.w2.weight\"]\n",
    "        ffn.w3_weight.data = weights[f\"{pfx}.ffn.w3.weight\"]\n",
    "\n",
    "        x_norm = ln2(x)\n",
    "        ffn_out = ffn(x_norm)\n",
    "        x = x + ffn_out\n",
    "\n",
    "    # ── Final Norm ────────────────────────────────────────────────────────────\n",
    "    ln_final = RMSNorm(d_model=d_model)\n",
    "    ln_final.weight.data = weights[\"ln_final.weight\"]\n",
    "    x = ln_final(x)\n",
    "\n",
    "    # ── LM Head (linear projection to vocab, no softmax — returns logits) ────\n",
    "    lm_head = Linear(in_features=d_model, out_features=vocab_size)\n",
    "    lm_head.weight.data = weights[\"lm_head.weight\"]\n",
    "\n",
    "    logits = lm_head(x)   # (batch, seq_len, vocab_size)\n",
    "\n",
    "    return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
